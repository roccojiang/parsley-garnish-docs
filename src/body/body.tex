\documentclass[../../main.tex]{subfiles}

\begin{document}

\TODO{
  Misc. things to write (might move to other sections):
  * Top-down parsers -- intro to left-recursion and the types of left recursion in PEG grammars
}

\TODO{
Not sure how to order things: lint rules first, or implementation?
}

\chapter{Lint Rules in \texttt{parsley-garnish}}

\TODO{Catalogue of lint rules implemented.}

\TODO{
Categorise these -- but also somehow split into the "simple" rules and the "complex" rules.
Simple rules can consist of a single heading, containing:
% TODO: Take inspiration from DLint paper, the Scala Refactoring master's thesis, HaRe papers
* Explanation of the rule
* Simple example to show a diagnostic, and a before and after if it's fixable
* How it's implemented in the code
* Proof (if applicable)
* Limitations

Simple rule ideas:
* Overly complex parser definitions
* Manually calling implicitSymbol instead of using the implicit

Not sure how to lay out the complex rules yet -- so far this is just the left-recursion removal rule.
The other complex rule(s) will likely share implementation details with the Parser/Func representation, so work from there.
}

\section{Avoid Redefining Existing Parsers}
\TODO{
* Catch cases when user manually writes out a parser that is already defined in the library
}

\section{Simplify Complex Parsers}
\TODO{
* Apply parser laws, re-using Parser and Func representations to do cool things <- should this be a separate rule?
}

\section{Ambiguous Implicit Conversions}
\epigraph{Heroin is just one letter away from heroine, and implicit conversions are the heroine we don't deserve.}{Jamie Willis, 2024}

\subsection*{Problem}

Implicit conversions are a powerful feature in Scala, allowing users to supply an argument of one type when another is expected, to reduce boilerplate.
As noted by \textcite{willis_design_2022}, implicit conversions are particularly useful for designing \textsc{dsl}s.
In the context of parser combinators, they introduce the usage of implicit conversions to automatically lift string and character literals into parsers in the \emph{Implicit Conversions} design pattern.
This eliminates the need to explicitly wrap these elements in combinators:
\scala{string("parsley") | string("garnish")} can now be expressed as just \scala{"parsley" | "garnish"}, more closely resembling the style of a \textsc{bnf} grammar.

The \emph{Implicit Lexer} pattern is a further specialisation of this approach, hiding the boilerplate of whitespace handling entirely within a \scala{lexer} object.
This design pattern allows whitespace handling to be encapsulated as private combinators within the \scala{lexer} object, which are then made available only through implicit conversions automatically applied by the Scala compiler.

However, due to their utility, implicit conversions are also an easily abused feature of Scala.
They can obscure the flow of the program, making it difficult to understand what the code is doing and potentially hiding side effects or costly operations.
A downside particularly relevant to Parsley is that implicit conversions often lead to confusing error diagnostics when the compiler is unable to resolve them.

One common issue arises from ambiguous implicits when there are multiple implicit conversions in scope.
Parsley provides \scala{stringLift} and \scala{charLift} combinators in the \texttt{parsley.syntax.character} package for the \emph{Implicit Conversions} pattern,
and exposes an \scala{implicitSymbol} combinator for lexers to use in the \emph{Implicit Lexer} pattern.
For novice users, it is easy to accidentally import both sets of these implicits, when it is likely that they only intended to use the \scala{implicitSymbol} implicit. % TODO: explain why lexer implicit supercedes character implicits?
For example, consider the following code snippet:
\begin{minted}{scala}
val p = 'g' ~> "arnish"
p.parse("garnish")
// [error] type mismatch;
//   found   : String("arnish")
//   required: parsley.Parsley[?]
//  Note that implicit conversions are not applicable because they are ambiguous:
//   both method stringLift in object character of type (str: String): parsley.Parsley[String]
//   and method implicitSymbol in class ImplicitSymbol of type (s: String): parsley.Parsley[Unit]
//   are possible conversion functions from String("arnish") to parsley.Parsley[?]
//    val p = 'g' ~> "arnish"
//                   ^^^^^^^^
\end{minted}

Here, the compiler provides a detailed error message indicating the ambiguity between two possible implicit conversions.
However, the compiler is not always able to report such issues clearly. For instance, switching the position of the intended implicit conversion results in a less helpful message:
\begin{minted}{scala}
val p = "garnis" <~ 'h'
p.parse("garnish")
// [error] value <~ is not a member of String
//    val p = "garnis" <~ 'h'
//            ^^^^^^^^^^^
\end{minted}

\subsection*{Solution}
Ideally, this issue would be addressed by implementing a lint-on-compile rule, which could annotate the compiler error message at the exact location of the issue.
If this were implemented as a compiler plugin, partial information available from the compiler stages before the error could potentially provide enough detail to identify the exact clashing implicits.
This approach would allow leveraging domain knowledge to update the error message with more useful Parsley-specific diagnostics.

Incidentally, WartRemover has a related lint rule for implicit conversions\footnote{\url{http://www.wartremover.org/doc/warts.html#implicitconversion}},
although it only targets the locations where implicit conversions are \emph{defined}, not where they are \emph{applied}.
Despite this limitation, it serves as a proof of concept demonstrating the feasibility of such an approach.

Unfortunately, Scalafix restricts usage to only syntactic rules on the bare \textsc{ast} or semantic rules that operate fully post-compilation.
Since the ambiguous implicit conversions will cause compilation failures, this lint must be implemented as a syntactic rule.
Consequently, the solution takes a different approach: estimating the presence of clashing implicits by examining their import statements within each scope.

\subsection*{Example}
\Cref{fig:ambiguous-implicits-example} extends the previous example to a full Scala source file following the \emph{Implicit Lexer} pattern,
but where the user has erroneously additionally imported the \scala{stringLift} implicit from the \emph{Implicit Conversions} pattern.
This results in the Scala compiler throwing an error on line 6 due to ambiguous implicits.
When run on this file, \texttt{parsley-garnish} will report a warning at line 3 similar to that shown in \cref{fig:ambiguous-implicits-warning}.

\begin{figure}[htbp]
\begin{minted}[frame=single,linenos]{scala}
object parser {
  import parsley.syntax.character.stringLift
  import lexer.implicits._

  val p = "garnis" <~ 'h'
}

object lexer {
  import parsley.token.Lexer, parsley.token.descriptions.LexicalDesc

  private val lexer = new Lexer(LexicalDesc.plain)
  val implicits = lexer.lexeme.symbol.implicits
}
\end{minted}
\caption{A minimal Parsley program which fails to compile due to ambiguous implicits in the \texttt{parser} object.}
\label{fig:ambiguous-implicits-example}
\end{figure}

\begin{figure}[htbp]
% TODO: maybe work out a nicer-looking frame for all these minted and fancyvrb environments
\begin{minted}[frame=single,fontsize=\small]{text}
warning: [AmbiguousImplicitConversions] This import may cause clashing implicit conversions:
* import parsley.syntax.character.stringLift at line 2
* import lexer.implicits._ at line 3
If this is the case, you may encounter confusing errors like 'method is not a member of String'.
To fix this, ensure that you only import a single implicit conversion.

  import lexer.implicits._
  ^^^^^^^^^^^^^^^^^^^^^^^^  
\end{minted}
\caption{The warning message produced by the \texttt{AmbiguousImplicitConversions} lint rule.}
\label{fig:ambiguous-implicits-warning}
\end{figure}

\subsection*{Implementation}
Unlike Java, Scala offers more flexibility with import statements, allowing them to appear anywhere in source files rather than just at the top.
Scala's import statements are lexically scoped, allowing their visibility to be limited to a single class, object, or function.
Additionally, Scala processes import statements in a top-down order within the file, further restricting their visibility, as scopes above an import cannot see the imports defined below them.


* Scalafix (via scalameta) provides a generic traversal of the AST: filter to find all import statements in top-down order
* This allows the scope to be lexically managed -- traversal in the same order that the compiler reads imports
* The ancestor AST node of an import statement is its enclosing scope
* Use ancestor information to determine which of the visited imports are in scope at that point

* to find stringLift: Pattern match to find if import is of form `import parsley.syntax.character.\_`
* to find implicit lexer: pattern match to find if there is an importee called `implicitSymbol` or if an import contains keywords `lexer` and `implicit(s)`

* if at any point in the traversal, both types of imports are in scope, report a warning

\section{Remove Explicit Usage of Implicit Conversions}

\section{Refactor to use Parser Bridges}
\TODO{
* This would be cool, idk if I have time though, but this should also piggyback off of Func
* the pos bridges don't actually exist, so we can ignore that case and just say its too much code synthesis
* shouldn't be too bad? idk
* indicate limitations that this will only work if the ADT is defined in the same file, in order to extend it
}

\section{Left Recursion Factoring}\label{sec:factor-leftrec}

\chapter{Implementation}

\TODO{
Non-terminal detection. This may get reworked/renamed since it's pretty specialised for leftrec rn, and in reality it's just trying to grab all the parsers. % <- where does this go?

Other util things?
ACTUALLY NEED TO DO: import combinators if they aren't already imported
}

% % Garnishing Parsec with Parsley 2018: Parsley is a deep embedding: the language is represented by objects and behaviours are realised as methods on an abstract trait. In Parsley’s case, the deep embedding provides methods for code generation and optimisation and classes for each core combinator. The advantage of a deep embedding over a shallow embedding is that it significantly easier to optimise using pattern matching on constructors instead of bytecode.

\section{Parser Representation}\label{sec:parser-representation}
Several of the more complex lint rules, most notably \cref{sec:factor-leftrec}, require manipulating parser combinators in a high-level manner.
It would be possible, but extremely cumbersome, to work directly with the generic Scala \textsc{ast} nodes provided by Scalameta.
Instead, it is very useful to represent parsers as an algebraic data type \textsc{adt} to simplify the manipulation of combinators.
This \namecref{sec:parser-representation} explores the motivation behind this and the design choices made in the implementation.
Use the \cref{sec:factor-leftrec} rule as a basis/context to demonstrate the utility of this representation.

Why do we want a parser representation? Because the left-recursion factoring is based on lawful transformations and rearrangements of parsers,
which are much easier to achieve and reason about when parsers are represented as a high-level \textsc{adt} rather than as raw Scala \textsc{ast} nodes.
This representation also then gives us for free the implementation for lint rules such as \emph{Simplify Complex Parsers} rule, which applies parser laws to simplify parser definitions.

Running example to motivate all requirements for the parser representation -- removing left recursion from the following simple parser:
\begin{minted}{scala}
lazy val expr: Parsley[String] = (expr, string("a")).zipped(_ + _) | string("b")
\end{minted}

\subsection{Detecting Parsers} % TODO: more descriptive title
Each source file is just a big AST.
A more useful representation will be a map of all parsers defined in that file, indexed by the symbol of the definition.
So we need to find AST nodes of interest which actually represent a Parser -- these are Scalameta \scala{Term}s.
Find val/var/def definitions which have a type that is inferred to have type \scala{Parsley[_]}.

Take our example:
\begin{minted}{scala}
lazy val expr: Parsley[String] = (expr, string("a")).zipped(_ + _) | string("b")
^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
val      sym expr: Parsley[_]    rhs of definition: convert this to Parser
\end{minted}
Convert rhs of definition to Parser.
In each document, build the map of symbol to parser.
This gets us all the non-terminals in the grammar defined within that file.

\subsection{Converting Scalameta Terms to the Parser \textsc{adt}}
Having identified the \textsc{ast} nodes which represent parsers, they need to be transformed into the appropriate \scala{Parser} representation.
This involves pattern matching on the \scala{Term} to determine which parser combinator it represents, and then constructing the appropriate \scala{Parser} instance.

Each Parser defines a partial function \scala{fromTerm} which creates an instance of that parser from the appropriate Scalameta \scala{Term}.
Use Scalafix's \scala{SymbolMatcher} to match tree nodes that resolve to a specific set of symbols.
This makes use of semantic information from SemanticDB, so we are sure that a \scala{<*>} is actually within the \scala{parsley.Parsley} package, rather than some other function with the same name.
This is a drawback of HLint, which is prone to false positives due to its reliance on syntactic information only.

Turning back to the running example, the top-level parser combinator in \scala{p} is the \scala{Choice} combinator:
\scala{expr} should either parse \scala{(expr, string("a")).zipped(_ + _)} or if that fails, \scala{string("b")}.
The \scala{Choice} parser combinator is represented as:
\begin{minted}{scala}
object Choice {
  val matcher = SymbolMatcher.normalized("parsley.Parsley.`|`", "parsley.Parsley.`<|>`")

  def fromTerm(implicit doc: SemanticDocument): PartialFunction[Term, Choice] = {
    case Term.ApplyInfix(p, matcher(_), _, Term.ArgClause(List(q), _)) =>
      Choice(p.toParser, q.toParser)
  }
}
\end{minted}
So \scala{Choice.fromTerm} looks for an infix application of the \scala{|} or \scala{<|>} combinator in the form of \scala{p | q}.
If it finds such a term, it constructs a \scala{Choice} node, and recurses on \scala{p} and \scala{q} to also convert them to parser nodes.

The rhs of \scala{expr} is represented as the Scalameta \scala{Term} (cleaned up to hide the large term representing the LHS of the choice):
\begin{minted}{scala}
Term.ApplyInfix(
  Term.Apply(...), // (expr, string("a")).zipped(_ + _)
  Term.Name("|"),
  Type.ArgClause(List()),
  Term.ArgClause(
    List(
      Term.Apply(
        Term.Name("string"),
        Term.ArgClause(List(Lit.String("b")), None)
      )
    ),
    None
  )
)
\end{minted}
It is easy to see that this matches the structure expected by \scala{Choice.fromTerm}, so the top-level node gets converted into a \scala{Choice} node.

In the end, \scala{expr} would be converted as so (cleaned up to be less verbose).
For now we are only concerned with the parser representation; we will touch upon how \scala{Function}s are represented in the next \namecref{sec:function-representation}.
\begin{minted}{scala}
// (expr, string("a")).zipped(_ + _) | string("b")
Choice(
  Zipped(Function(_ + _), List(NonTerminal(expr), Str(a))),
  Str(b)
)
\end{minted}

\subsection{Building New Parsers From Existing Parsers}
Now that we have parsers represented as an \textsc{adt}, we can easily build new parsers from existing parsers.
This is crucial for the left-recursion factoring rule, which ``unfolds'' parsers into separate parsers representing the left-recursive and non-left-recursive parts.
These are then recombined to form parsers which are free from left recursion.

Make this even easier by utilising Scala's ability to define infix operators, define them as extension methods on the \scala{Parser} trait.
For example:
\begin{minted}{scala}
implicit class ParserOps(private val p: Parser) extends AnyVal {
  def <*>(q: Parser): Parser = Ap(p, q)
  def <|>(q: Parser): Parser = Choice(p, q)
  def map(f: Function): Parser = FMap(p, f)
}
\end{minted}
%
This makes it more ergonomic to manipulate parsers, it's like we're writing Parsley code itself.
A small example snippet from the \scala{unfold} method on the \scala{Ap} parser:
\begin{minted}{scala}
val lefts = {
  val llr = pl.map(flip) <*> q
  val rlr = pe.map(f => ql.map(composeH(f))).getOrElse(Empty)
  llr <|> rlr
}
\end{minted}
Notice how the code closely resembles the high-level description of the transformation, using \scala{<*>}, \scala{<|>}, \scala{map}, operators.

\subsection{Simplifying Parsers Using Parser Laws}
Once all the unfolded parsers have been recombined, the raw output is very noisy and difficult to read.
Again, ignore the functions, these will be covered in \cref{sec:function-representation}.
\begin{minted}{scala}
lazy val expr: Parsley[String] = chain.postfix(
  empty | (empty.map(a => b => a + b) | empty <*> expr) <*> string("a") | string("b") | empty
)(
  (empty.map(FLIP) <*> expr | pure(ID).map(COMPOSE(a => b => a + b))).map(FLIP) <*> string("a")
    | empty | empty
)
\end{minted}
%
For human readability of the transformed output, it is therefore important to simplify the parser as much as possible.
This is another key motivation for the parser representation, bringing static inspectability.
Similar to the high-level optimisations done in~\cite{willis_staged_2023} using parser laws based on applicative, alternative, selective.
\textcite{willis_parsley_2023} provides the most comprehensive list of parser laws which Parsley adheres to.

\begin{figure}[htbp]
\centering
\begin{gather}
  % Functor
  \text{\scala{p.map(f).map(g) = p.map(g compose f)}} \label{eqn:functor-comp} \\
  % Applicative functor
  \text{\scala{pure(f) <*> pure(x) = pure(f(x))}} \label{eqn:app-homomorphism} \\
  \text{\scala{pure(f) <*> x = x.map(f)}} \label{eqn:app-fmap} \\
  % Alternative applicative functor
  \text{\scala{empty | u = u}} \label{eqn:alt-left-neutral} \\
  \text{\scala{u | empty = u}} \label{eqn:alt-right-neutral} \\
  \text{\scala{pure(x) | u = pure(x)}} \label{eqn:alt-left-biased-choice} \\
  \text{\scala{empty <*> u = empty}} \label{eqn:alt-empty-absorb} \\
  \text{\scala{empty.map(f) = empty}} \label{eqn:alt-fmap-absorb}
\end{gather}
% I've wanted more fine-grained control, so instead of using cleveref I've manually written out the references -- TAKE CARE to keep them in the same order as the equations
% \caption{Functor~\cref{eqn:functor-comp}, Applicative~\cref{eqn:app-homomorphism,eqn:app-fmap}, and Alternative~\cref{eqn:alt-left-neutral,eqn:alt-right-neutral,eqn:alt-left-biased-choice,eqn:alt-empty-absorb,eqn:alt-fmap-absorb} laws.}
\caption{Functor~(\ref{eqn:functor-comp}), Applicative~(\ref{eqn:app-homomorphism}, \ref{eqn:app-fmap}), and Alternative~(\ref{eqn:alt-left-neutral}--\ref{eqn:alt-fmap-absorb}) laws.}
\label{fig:parser-laws}
\end{figure}

Most of the laws in \cref{fig:parser-laws} have already been shown to hold for Parsley in~\cite{willis_garnishing_2018}.
An additional proof for \cref{eqn:alt-fmap-absorb} can be found in \cref{appendix:parser-law-proofs}.

Example by hand.
In the example we can see that the most noise results from the \scala{empty} combinators. % TODO: add why?
Tidying those up first:
\begin{minted}{scala}
lazy val expr: Parsley[String] = chain.postfix(
  empty | empty | string("b") | empty
)(
  (pure(ID).map(COMPOSE(a => b => a + b))).map(FLIP) <*> string("a")
    | empty | empty
)
\end{minted}
%
\scala{empty | empty | string("b") | empty} simplifies to just \scala{string("b")} via \cref{eqn:alt-left-neutral,eqn:alt-right-neutral}.
\begin{minted}[escapeinside=\%\%]{scala}
    (pure(ID).map(COMPOSE(a => b => a + b))).map(FLIP) <*> string("a")
% \proofstep{\cref{eqn:app-homomorphism,eqn:app-fmap}} %
    pure(COMPOSE(a => b => a + b)(ID)).map(FLIP) <*> string("a")
% \proofstep{\cref{eqn:app-homomorphism,eqn:app-fmap}} %
    pure(FLIP(COMPOSE(a => b => a + b)(ID))) <*> string("a")
% \proofstep{\cref{eqn:app-fmap}} %
    string("a").map(FLIP(COMPOSE(a => b => a + b)(ID)))
\end{minted}
So final result:
\begin{minted}{scala}
val f: Function = FLIP(COMPOSE(a => b => a + b)(ID))
lazy val expr: Parsley[String] = chain.postfix(string("b"))(string("a").map(f))
\end{minted}

\subsubsection{Implementation}
Bottom-up traversal to apply peephole optimisations based on parser laws.
For maintainability, code readability, and extensibility: decouple the application of transformation function from recursive traversal through the datatype.
Hand-written traversal, but inspired by the generic traversal techniques in \cite{mitchell_uniform_2007}.
Make ADT sealed so the compiler will warn if we miss on adding a new case: this is still more error prone than generically deriving, but we don't want to introduce a new dependency on Shapeless library (it's not complex enough to warrant bringing in a dependency for generic derivation).

% TODO
% \TODO{
% % Garnishing Parsec with Parsley 2018: Parsley is a deep embedding: the language is represented by objects and behaviours are realised as methods on an abstract trait. In Parsley’s case, the deep embedding provides methods for code generation and optimisation and classes for each core combinator. The advantage of a deep embedding over a shallow embedding is that it significantly easier to optimise using pattern matching on constructors instead of bytecode.
% Representation of Parsley combinators in \texttt{parsley-garnish}. Compare with approach in Scala Parsley, take cues from the 2018 paper.
% * Approach to composites? Need to think about this.
%   * For LeftRec: Parse ASTs into a small group of core combinators, but we also need to represent composite combinators as their own case classes -- recombine/"simplify" after analysis is concluded, it doesn't really matter if we completely change what combinators are used as long as semantic meaning is preserved.
%   * For others: probably need to parse directly into composite combinators, since we don't want to destructively modify what combinators have been used.
% * Optimisations: for us, the goal is human readability, so this is interesting to compare to the paper. Lots of similar stuff actually, like top-down peephole optimisations utilising parser laws (I think I do it this way? Need to double check).
%   * For cleanliness to isolate boilerplate: https://blog.sumtypeofway.com/posts/introduction-to-recursion-schemes.html -- we don't have a generic traversal, but we can decouple the recursive application of a given partial function from the actual pf itself (I've called it .transform for the Parser class)
%     % https://ndmitchell.com/downloads/paper-uniform_boilerplate_and_list_processing-30_sep_2007.pdf covers the important stuff wrt: bottom-up traversal, rewrite to normal form - since our rules have RHS that appear on LHS
% }

\section{Function Representation}\label{sec:function-representation}
From the running example it is hopefully becoming clear that functions also need to be simplified, for the same reasons that parsers need to be simplified.

\TODO{
Abstraction built over scalafix/meta ASTs to represent functions.
Allows us to statically evaluate function composition/flipping etc, so it doesn't turn into one big mess -- again, human readability of the transformed output is the goal.
% https://hugopeters.me/posts/16/ -- should I switch to de bruijn indices? Bound variables get instantiated only at the end, don't have to worry about alpha conversion (relation to Barendregt's convention if needed??)
Abstraction is again an ADT as a lambda calculus, but with parameter lists so not everything is curried.
\^ idk, this is still a work-in-progress. Seems that there might not be enough time to uncurry the leftrec analysis so this design decision might not be super important.
Representation as a lambda calc has allocation overhead, but greatly simplifies function evaluation via beta reduction, instead of having to deal with high-level representations of compose/id (not too bad tbh) and flip (annoying).
Also attempted to make it typed but that didn't go so well with Scala's limitations on type inference.

* Extracting method arguments (alongside their types) is very painful
* Need to unify information from signature (within symbolinformation) and synthetics
  * synthetics exist in certain cases: .apply methods, showing the concrete type of a generic argument, implicit conversions
    * from https://scalacenter.github.io/scalafix/docs/developers/semantic-tree.html: SemanticTree is a sealed data structure that encodes tree nodes that are generated by the compiler from inferred type parameters, implicit arguments, implicit conversions, inferred .apply and for-comprehensions.

% TODO: FIGURE OUT ALL THE IMPORTANT CASES TO COVER:
* map, lift (implicit and explicit), zipped, (.as perhaps?)
  -- these should surely boil down into two cases: (x, y).xxx(f) and xxx(f, x, y)
  * named function literals (val)
  * named method literals (def)
  * anonymous functions i.e. lambdas
  * functions with placeholder syntax
  * apply methods of case classes - symbol will tell its a class signature so we use this as a clue to look at synthetics???
* generic bridges -- I reckon the information will probably show up in synthetics again

* Don't have full access to type information - can do more work here theoretically, but its difficult and error-prone
* So we don't model a typed lambda calculus, just have it untyped

Approaches - AVOIDING capture via substitution
* Substitution approaches
  * De Bruijn indices - inefficient to open/close terms so much - De Bruijn levels as an alternative
  * HOAS
* Normalisation by evaluation
}

\end{document}
