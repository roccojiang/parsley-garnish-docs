\documentclass[../../main.tex]{subfiles}

\begin{document}

\section{Function Representation}\label{sec:function-representation}
\TODO{
  This section is about simplifying in the general domain, so really Squid can do all of this? Still an interesting approach ig
  And shows a shortcoming of scalameta quasiquotes
}

\Cref{sec:parser-representation} showed that it is useful to lift \textsc{ast} nodes corresponding to parser terms into an intermediate \scala{Parser} representation.
Crucially, \cref{sec:simplify-parsers} demonstrates how this representation lends itself to reducing parsers to a simpler form.
This \namecref{sec:function-representation} extends this idea to representing functions in a similar, statically-inspectable manner, and explores why this machinery is not just desirable but necessary for \texttt{parsley-garnish}.

% TODO: there's some decent stuff in Ch8 of david davie's masters thesis briefly summing up how to articulate related things in a succinct manner
\subsection{Representing Functions}
Utilising parser laws as a basis for term simplification for the \scala{Parser} \textsc{adt} has managed to tame the jumble of parser combinators resulting from the left-recursing factoring transformation~\cref{sec:factor-leftrec}.
However, the functions passed to lifting combinators such as \scala{map} have so far been treated as opaque, and have not been subject to the same simplifications as parsers.

Continuing from the running example from the previous \namecref{sec:parser-representation}, the function passed to the \scala{map} combinator has ended up in an unnecessarily complex form:
\begin{minted}{scala}
val f: Function = flip(compose(a => b => a + b)(identity))
\end{minted}
%
This is because the transformations on parsers during left-recursion factoring involve mapping over parsers with higher-order functions such as \scala{flip} to reverse the order of arguments, and \scala{compose} to compose functions.
These are used to ensure that unfolded parsers are recombined in a lawful manner, but results in a lot of syntactic noise.
Yet again, leaving the transformed terms in this form would be unacceptable for the linter.
It is vital that these functions are partially evaluated and \emph{normalised} into a semantically equivalent but visually simpler form.
To achieve this, a representation of functions is needed that is amenable to the same kind of static inspection as the \scala{Parser} \textsc{adt}.

\subsubsection{Defunctionalisation}
In the same way that representing parsers as datatypes allows them to be statically inspectable, the aforementioned higher-order functions can also be rendered as datatypes to allow them to be analysed in a similar manner.
This is an example of \emph{defunctionalisation}, a technique to eliminate higher-order functions by transforming them into specialised first-order equivalents~\cite{reynolds_defunc_1972,danvy_defunctionalization_2001}.

Ideally for type safety, this could be represented as a generalised algebraic data type (\textsc{gadt})~\cite{cheney_gadt_2003} with a type parameter documenting the type that the function term represents:
\begin{minted}{scala}
trait Function[+T]
case class Identity[A]() extends Function[A => A]
case class Flip[A, B, C]() extends Function[(A => B => C) => B => A => C]
case class Compose[A, B, C]() extends Function[(B => C) => (A => B) => A => C]
\end{minted}

\subsubsection{Lambda Calculus}
The defunctionalised constructs alone are not enough to fully represent the functions passed to combinators.
There needs to be a way to apply them to arguments, and to represent the arguments themselves -- the core language to do so is the lambda calculus~\cite{church_lambda_1936}.
In the $\lambda$-calculus, each function only takes one argument, and multi-argument functions are represented as a chain of single-argument functions: this is known as \emph{currying}.
In Scala, functions are allowed to have multiple parameter lists to make them curried, although uncurried functions are preferred for performance reasons.
Since these functions will be transformed from Scala code and back, it is desirable to maintain a high-level equivalence between these two representations.
Thus, \cref{fig:lambda-calculus} shows the $\lambda$-calculus extended to represent multi-argument functions using $n$-ary abstraction and application.

\begin{figure}
% annoying \enspace hack to get operators aligned along the centre
\begin{align*}
M, N &\mathrel{::=} x & \text{(variable)} \\
&\mathrel{\enspace\mid\enspace} \lambda(x_1, \ldots, x_n).M & \text{($n$-ary abstraction)} \\
&\mathrel{\enspace\mid\enspace} M(N_1, \ldots, N_n) & \text{($n$-ary application)}
\end{align*}
\caption{The untyped $\lambda$-calculus extended with $n$-ary abstraction and application.}
\label{fig:lambda-calculus}
\end{figure}

\subsubsection{The Function \textsc{adt}}
The syntactic representation of functions is thus represented as the uni-typed \scala{Function} \textsc{adt} in \cref{fig:function-adt}.
The \scala{Lambda} trait is a subset of the entire \scala{Function} trait, representing $\lambda$-terms, with the addition of a \scala{Translucent} term to encapsulate open terms holding a \scala{scala.meta.Term} which cannot be evaluated further.

\begin{figure}
\begin{minted}{scala}
trait Function

// Core expression language
trait Lambda extends Function
case class Abs(xs: List[Var], f: Function) extends Lambda
case class App(f: Function, xs: List[Function]) extends Lambda
case class Var(name: VarName) extends Lambda
case class Translucent(t: Term, env: Map[VarName, Function]) extends Lambda

// Defunctionalised higher-order functions
case object Identity extends Function
case object Flip extends Function
case object Compose extends Function
...
\end{minted}
\caption{The \scala{Function} \textsc{adt} for representing functions.}
\label{fig:function-adt}
\end{figure}

The original intention was to represent \scala{Function} as a type-parameterised \textsc{adt} for improved type safety, where \scala{Lambda} would use a variant of the simply typed $\lambda$-calculus.
This would've also allowed \scala{Parser} to be parameterised by the result type of the parser.
However, attempting to implement this ran into two main hurdles:
\begin{itemize}
  \item \scala{Var} and \scala{Translucent} terms would need to be created with concrete type parameters of their inferred types. Scalafix's semantic \textsc{api} is not powerful enough to guarantee that all terms can be queried for their inferred types -- in fact, the built-in Scalafix rule \emph{Explicit Result Types} calls the Scala 2 presentation compiler to extract information like this\footnote{\url{https://github.com/scalacenter/scalafix/issues/1583}}. This solution is not ideal as it is brittle and breaks Scalafix's cross-compatibility promises.
  \item Scala 2's type inference for \textsc{gadt}s is less than ideal, requiring extra type annotations and unsafe casts which ultimately defeat the original purpose of type safety. This situation is improved, although not completely solved, in Dotty~\cite{parreaux_towards_2019} -- but Scalafix does not yet support Scala 3.
\end{itemize}

\subsection{Achieving Normalisation}\label{sec:normalisation-approach}
\TODO{
% TODO: this is based on NbE ideas!! move accordingly
Partial evaluation and normalisation are related concepts: it is useful to view normalisation as statically evaluating as many terms as possible, but since not all terms have known values, the expression cannot be fully evaluated to a result value.
Normalisation can thus be viewed simply as a process of evaluation, but in the presence of unknown terms.
This \namecref{sec:normalisation-approach} first explains the traditional notion of normalisation by rewriting, before introducing normalisation by evaluation as a more elegant and efficient alternative.
}

\subsubsection{Reduction-Based Normalisation}
In the $\lambda$-calculus, terms are reduced in size by applying $\beta$-reduction: \cref{fig:beta-reduction} shows how this can be defined for the $n$-ary $\lambda$-calculus.
Unlike the standard $\lambda$-calculus, reduction will only take place if the expected number of arguments in $\mathbf{x} = x_1, \ldots, x_n$ are equal to the number of arguments in $\mathbf{N} = N_1, \ldots, N_n$; otherwise, the term is left as is.
This is therefore a \emph{directed} notion of reduction, which can be implemented as a term-rewriting system, in a similar way to how \scala{Parser} terms are simplified.

The goal is to achieve beta normal form ($\beta$-\textsc{nf}) by allowing $\beta$-reduction to occur deep inside $\lambda$-terms, in all redexes of a term, until no more reductions can be made.
Normally in the untyped $\lambda$-calculus, $\beta$-reduction as a rewriting rule does not manage to be strongly normalising as desired, and may lead to non-terminating rewrites.
However, despite the \scala{Lambda} representation itself being untyped, all \scala{Lambda} terms are converted from valid typed Scala expressions.
Scala 3 is modelled on the theoretic foundations of the Dependent Object Types (\textsc{dot}) calculus, which has been shown to be strongly normalisable~\cite{wang_strong_2017}.
Although not proven for Scala 2, it is reasonable to assume that the same result holds, at least for the subset of expressions that are represented in the \scala{Lambda} representation.

\begin{figure}[htbp]
\begin{equation*}
(\lambda \mathbf{x} . M) \mathbf{N} \rightarrow_\beta M[\mathbf{N}/\mathbf{x}] \hspace{3em} (\text{if } | \mathbf{x} | = | \mathbf{N} | )
\end{equation*}
\caption{The $\beta$-reduction rule for the $n$-ary lambda calculus.}
\label{fig:beta-reduction}
\end{figure}

In \cref{fig:beta-reduction}, the syntax $M[N/x]$ denotes term substitution, where all free occurrences of $x$ in $M$ are replaced with $N$.
Substitution must avoid \emph{variable capture}, when $N$ contains free variables that are bound in the scope where $x$ is found~\cite{van-bakel_tsfpl_2022}.
Capture-avoiding substitution is a notoriously tricky and subtle problem.
There are a plethora of different approaches to solve this, mostly boiling down to how variable names get represented:

\paragraph{Naïve substitution}
Representing variable names as strings is the most straightforward approach in terms of understandability.
Naïvely substituting these terms, however, seems logically simple but can be very tricky to get right.
This approach requires calculating the free variables in a scope before performing substitution, renaming bound variables if it would lead to variable capture.
This is not used in any real implementation of the $\lambda$-calculus due to its inefficiency, as it requires traversing the term tree multiple times.

\paragraph{Barendregt's convention}
Renaming all bound variables to be unique satisfies \textbf{Barendregt's convention}~\cite{barendregt_lambda_1984}, which removes the need to check for variable capture during substitution.
However, variables still have to be renamed during substitution -- this administrative renaming has relatively high performance overhead and chews through a scarily large number of fresh variable names.
This approach can be massively optimised though: the Haskell \textsc{ghc} compiler uses a technique dubbed ``the Rapier''~\cite{peytonjones_secrets_2002}, maintaining invariants to avoid renaming on substitution when unnecessary.
Unfortunately, maintaining the invariants to keep this transformation correct becomes very difficult~\cite{maclaurin_thefoil_2023}.

\paragraph{Nameless and hybrid representations}
Nameless representations like \emph{De Bruijn indices}~\cite{debruijn_lambda_1972} eschew names entirely, instead representing variables as the number of binders between the variable and its binding site.
Although an elegant representation, De Bruijn terms are notoriously difficult to work with, as they are not easily human-readable.
% https://proofassistants.stackexchange.com/questions/900/when-should-i-use-de-bruijn-levels-instead-of-indices
Furthermore, performing substitutions with De Bruijn terms has an overhead as variable positions have to be shifted.
To avoid this, hybrid representations combining named and nameless representations exist~\cite{mcbride_imnotanumber_2004,chargueraud_locally_2012}, but they become rather complex solutions for what should be a relatively simple $\lambda$-calculus implementation for \texttt{parsley-garnish}'s needs.

% \scala{Function} has one binding construct -- the n-ary abstraction. This introduces variables and the need for capture-avoiding substitution.

\subsubsection{Normalisation by Evaluation}
An interesting alternative approach stems from a notion of \emph{reduction-free} normalisation, based on an undirected notion of term equivalence, rather than directed reduction.
% TODO: this is good framing for using this to also see if parsers are equivalent
\emph{Normalisation by Evaluation} (\textsc{nbe}) evaluates syntactical terms into a semantic model, which is then \emph{reified} back into the syntactic model~\cite{filinski_nbe_2004}.
This denotational model (denoted by $\text{\textlbrackdbl} - \text{\textrbrackdbl}$) is specifically constructed to be \emph{residualising}, meaning that terms can be extracted out into the original syntactic representation.

% Use HOAS to leverage the meta-language (i.e. Scala) as our semantic model -- blazingly fast! (theoretically lol)
% No rewriting = no worrying about avoiding capture.
% Shift that burden onto the scala compiler.

\begin{figure}
\begin{equation*}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12cYAPHYAMoBPMDjo8AviAml0mXPkIoATOSq1GLNp259BMALZ1RWAMZwpE9TCgBzeEVAAzAE4QDSMiBwQkARmo4AAssJxwkAFpVDWZWRA4uXn4dJIYAIxc6UwBrKDSGCQACCMKUvTKcFwys3PzLWRBXd39qH09AkLDI6IY6NJgGAAV5PAI2Bhgu6npY7US+XGAXGFChSwoJIA
\begin{tikzcd}
  \text{Syntax} \arrow[rr, "\text{\textlbrackdbl} - \text{\textrbrackdbl}", shift left=2] &  & \text{Semantics} \arrow[ll, "\textit{reify}", shift left=2]
\end{tikzcd}
\end{equation*}
\vspace{0.5ex}
\begin{equation*}
\textit{normalise} = \textit{reify} \circ \text{\textlbrackdbl} - \text{\textrbrackdbl}
\end{equation*}
\caption{Normalisation by evaluation in a semantic model.}
\end{figure}

\subsection{Converting Scalameta Terms to the Function \textsc{adt}}
Three cases:
\begin{enumerate}
    \item \scala{Term.Function} nodes, representing lambda expressions
    \item \scala{Term.AnonymousFunction} nodes, representing lambda expressions using placeholder syntax
    \item Any other term, which will be $eta$-expanded if possible
\end{enumerate}

\subsubsection{Lambda Expressions}
Often found as relatively simple functions to glue together parsers, or transform the result of a parser:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(char => char.toInt)
\end{minted}

Choose a particularly convoluted example:
\begin{minted}{scala}
a => (a, b) => a + b
\end{minted}
In this example, the \scala{a} in the function body refers to the \scala{a} in the second parameter list, as it shadows the \scala{a} in the first parameter list.

Conversion ``flattens'' the lambda expression into a chain of n-ary abstractions, with the final term being the body of the lambda.
Because we lose scoping information this way, perform $\alpha$-conversion on variables.
% TODO: this is "gensym" approach
The body becomes a \scala{Translucent} term.

\begin{lstlisting}
\(_l1). \(_l2, _l3). Translucent(%\textbf{\_l2 + \_l3}%, env = {%\textbf{\_l1}% -> _l1, %\textbf{\_l2}% -> _l2, %\textbf{\_l3}% -> _l3})
\end{lstlisting}
Values in bold are Scalameta tree nodes, so the body term's environment maps \scala{Term.Name} nodes to their corresponding variable terms.

Fold on parameter lists

\subsubsection{Placeholder Syntax}
Scala supports placeholder syntax for terseness, so the earlier parser can be rewritten as:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(_.toInt)
\end{minted}

Placeholder syntax only results in a single parameter list, so this just becomes a single n-ary lambda

\subsubsection{Eta-Expansion}
\begin{minted}{scala}
case class AsciiCode(code: Int)
object AsciiCode extends ParserBridge1[Char, AsciiCode] {
  def apply(char: Char): AsciiCode = AsciiCode(char.toInt)
}
val asciiCode = AsciiCode(item) // desugars to item.map(AsciiCode.apply)
\end{minted}

\subsection{Normalising Function Terms}
\begin{figure}[htbp]
\begin{equation*}
% Created here (but modified slightly) https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDnGADx2AAJAPIBBAMoBfEJNLpMufIRQBmCtTpNW7Lj35CxUgHq6+AsADNps+djwEiZAEwaGLNok7czwAGLMwAGN7MGs5EAw7JSI1Fxo3bU9TfQAZegBbACMoejDbRQcUcnV4rQ8vPQFxGHT6MDxAuDyIhRCiYrjNdx1vfXEAT3r6XmsNGCgAc3giUAsAJwh0pGKQHAgkMi7EirMcARg3aRpGekyYRgAFVujPRhgLHBlw+cWNmjWkNS3y5L2BOZgWAs-SOIBOZ0u10KYPujxsIBeS0QTne60QKwSP16f2AAIsd2CoPB5yuUWhdweT1mCyRKNWaK+mJ6lT2wEgc1qjCwcBgkgABABePm-VkAoEg4UcQJYOaBSUs-aHSXS2Xy3b-e4EnD8kDHU4kqHKEBzLATAAWcOeNKQdI+iE2XLA5RycDN41133YyGQfIAtHzKJQZJRJEA
\begin{tikzcd}[sep=5em]
  \text{Semantics} & \text{HOAS} \arrow[rr, "\texttt{eval}"]                                                                                                                                         &  & \text{HOAS}^\text{nf} \arrow[dd, "\texttt{reify}"] \\
                   &                                                                                                                                                                                 &  &                                                    \\
  \text{Syntax}    & \text{Function} \arrow[uu, "\texttt{reflect}"] \arrow[rr, "\substack{\texttt{normalise} \\ =\ \texttt{reify}\ \circ\ \texttt{eval}\ \circ\ \texttt{reflect}}"'] \arrow[rruu, "{\text{\textlbrackdbl} - \text{\textrbrackdbl}}", dashed] &  & \text{Lambda}                                     
\end{tikzcd}
\end{equation*}
\caption{Normalisation by evaluation for the \scala{Function} datatype.}
\end{figure}

\begin{minted}{scala}
trait HOAS
case class HAbs(n: Int, f: List[HOAS] => HOAS) extends HOAS
case class HApp(f: HOAS, xs: List[HOAS]) extends HOAS
case class HVar(name: VarName) extends HOAS
case class HTranslucent(t: Term, env: Map[VarName, HOAS]) extends HOAS
\end{minted}
%
Abs represents n-ary $\lambda$-abstraction; App is n-ary function application; Var represents opaque free variables.
Translucent is treated as opaque during normalisation, but contains normalisable terms in an environment, where they are substituted back in after normalisation.

\subsubsection{Reflection}

\begin{minted}{scala}
trait Function {
  def reflect: HOAS = {
    def reflect0(func: Function, boundVars: Map[Var, HOAS]): HOAS = func match {
      case Abs(xs, f) =>
        HAbs(xs.size, vs => reflect0(f, boundVars ++ xs.zip(vs)))
      case App(f, xs) =>
        HApp(reflect0(f, boundVars), xs.map(reflect0(_, boundVars)))
      case v @ Var(name) =>
        boundVars.getOrElse(v, HVar(name))
      case Translucent(term, env) =>
        HTranslucent(term, env.mapValues(reflect0(_, boundVars)))

      case Id => HAbs(f => f)
      case Flip => HAbs(f => HAbs(x => HAbs(y => HApp(HApp(f, y), x))))
      case Compose => HAbs(f => HAbs(g => HAbs(x => HApp(f, HApp(g, x)))))
    }

    reflect0(this, Map.empty)
  }
}
\end{minted}

\subsubsection{Evaluation}
\begin{minted}{scala}
trait HOAS {
  def eval: HOAS = this match {
    case HAbs(n, f) => HAbs(n, x => f(x).eval)
    case HApp(f, x) => f.whnf match {
      case HAbs(_, g) => g(x).eval
      case g          => HApp(g.eval, x.map(_.eval))
    }
    case HTranslucent(t, env) => HTranslucent(t, env.map { case (k, v) => k -> v.eval })
    case HTranslucent(t, env) => HTranslucent(t, env.mapValues(_.eval))
    case _ => this
  }

  private def whnf: HOAS = this match {
    case HApp(f, x) => f.whnf match {
      case HAbs(_, g) => g(x).whnf
      case g          => HApp(g, x)
    }
    case HTranslucent(t, env) => HTranslucent(t, env.mapValues(_.whnf))
    case _ => this
  }
}
\end{minted}

\subsubsection{Reification}
\begin{minted}{scala}
trait HOAS {  
  def reify: Function = this match {
    case HAbs(n, f) =>
      val params = (1 to n).map(_ => Var.fresh()).toList
      Abs(params, f(params.map(x => HVar(x.name))).reify)
    case HApp(f, xs) => App(f.reify, xs.map(_.reify))
    case HVar(name) => Var(name)
    case HTranslucent(t, env) => Translucent(t, env.mapValues(_.reify))
  }
}
\end{minted}

\subsection{Converting Functions back to Scalameta Terms}
Surprise bitches same shit again, quasiquotes ftw

\subsection{Further Work?}
% TODO: probably move this to the end of the document
Eta reduction -- this is more complicated than in Haskell since Scala has special syntax % https://docs.scala-lang.org/scala3/book/fun-eta-expansion.html
Partial evaluation, not just normalisation (if we reduce to fully closed terms 1+1 can we get it to evaluate to 2? -- except currently this would be a Translucent term)

\TODO{
TODO:
Abstraction built over scalafix/meta ASTs to represent functions.
Allows us to statically evaluate function composition/flipping etc, so it doesn't turn into one big mess -- again, human readability of the transformed output is the goal.
% https://hugopeters.me/posts/16/ -- should I switch to de bruijn indices? Bound variables get instantiated only at the end, don't have to worry about alpha conversion (relation to Barendregt's convention if needed??)
Abstraction is again an ADT as a lambda calculus, but with parameter lists so not everything is curried.
\^ idk, this is still a work-in-progress. Seems that there might not be enough time to uncurry the leftrec analysis so this design decision might not be super important.
Representation as a lambda calc has allocation overhead, but greatly simplifies function evaluation via beta reduction, instead of having to deal with high-level representations of compose/id (not too bad tbh) and flip (annoying).
Also attempted to make it typed but that didn't go so well with Scala's limitations on type inference.

* Extracting method arguments (alongside their types) is very painful
* Need to unify information from signature (within symbolinformation) and synthetics
  * synthetics exist in certain cases: .apply methods, showing the concrete type of a generic argument, implicit conversions
    * from https://scalacenter.github.io/scalafix/docs/developers/semantic-tree.html: SemanticTree is a sealed data structure that encodes tree nodes that are generated by the compiler from inferred type parameters, implicit arguments, implicit conversions, inferred .apply and for-comprehensions.

* Don't have full access to type information - can do more work here theoretically, but its difficult and error-prone
* So we don't model a typed lambda calculus, just have it untyped

Approaches - AVOIDING capture via substitution
* Substitution approaches
  * De Bruijn indices - inefficient to open/close terms so much - De Bruijn levels as an alternative
  * HOAS
* Normalisation by evaluation
}

\end{document}
