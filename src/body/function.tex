\documentclass[../../main.tex]{subfiles}

\begin{document}

\section{Function Representation}\label{sec:function-representation}
\TODO{
  This section is about simplifying in the general domain, so really Squid can do all of this? Still an interesting approach ig
  And shows a shortcoming of scalameta quasiquotes
}

\Cref{sec:parser-representation} showed that it is useful to lift \textsc{ast} nodes corresponding to parser terms into an intermediate \scala{Parser} representation.
Crucially, \cref{sec:simplify-parsers} demonstrates how this representation lends itself to reducing parsers to a simpler form.
This \namecref{sec:function-representation} extends this idea to representing functions in a similar, statically-inspectable manner, and explores why this machinery is not just desirable but necessary for \texttt{parsley-garnish}.

% TODO: there's some decent stuff in Ch8 of david davie's masters thesis briefly summing up how to articulate related things in a succinct manner
\subsection{Representing Functions}
Utilising parser laws as a basis for term simplification for the \scala{Parser} \textsc{adt} has managed to tame the jumble of parser combinators resulting from the left-recursing factoring transformation~\cref{sec:factor-leftrec}.
However, the functions passed to lifting combinators such as \scala{map} have so far been treated as opaque, and have not been subject to the same simplifications as parsers.

Continuing from the running example from the previous \namecref{sec:parser-representation}, the function passed to the \scala{map} combinator has ended up in an unnecessarily complex form:
\begin{minted}{scala}
val f: Function = flip(compose(a => b => a + b)(identity))
\end{minted}
%
This is because the transformations on parsers during left-recursion factoring involve mapping over parsers with higher-order functions such as \scala{flip} to reverse the order of arguments, and \scala{compose} to compose functions.
These are used to ensure that unfolded parsers are recombined in a lawful manner, but results in a lot of syntactic noise.
Yet again, leaving the transformed terms in this form would be unacceptable for the linter.
It is vital that these functions are partially evaluated and \emph{normalised} into a semantically equivalent but visually simpler form.
To achieve this, a representation of functions is needed that is amenable to the same kind of static inspection as the \scala{Parser} \textsc{adt}.

\subsubsection{Defunctionalisation}
In the same way that representing parsers as datatypes allows them to be statically inspectable, the aforementioned higher-order functions can also be rendered as datatypes to allow them to be analysed in a similar manner.
This is an example of \emph{defunctionalisation}, a technique to eliminate higher-order functions by transforming them into specialised first-order equivalents~\cite{reynolds_defunc_1972,danvy_defunctionalization_2001}.

Ideally for type safety, this could be represented as a generalised algebraic data type (\textsc{gadt})~\cite{cheney_gadt_2003} with a type parameter documenting the type that the function term represents:
\begin{minted}{scala}
trait Defunc[+T]
case class Identity[A]() extends Defunc[A => A]
case class Flip[A, B, C]() extends Defunc[(A => B => C) => B => A => C]
case class Compose[A, B, C]() extends Defunc[(B => C) => (A => B) => A => C]
\end{minted}

\TODO{
  Facilitates more high-level parser simplifications
}

\subsubsection{Lambda Calculus}
The defunctionalised constructs alone are not enough to fully represent the functions passed to combinators.
There needs to be a way to apply them to arguments, and to represent the arguments themselves -- the core language to do so is the lambda calculus~\cite{church_lambda_1936}.
% TODO: clunky wording
Furthermore, once the defunctionalised functions have exhausted their usefulness in high-level parser simplifications, they also need to be lowered to $\lambda$-terms to be normalised.
In the $\lambda$-calculus, each function only takes one argument, and multi-argument functions are represented as a chain of single-argument functions: this is known as \emph{currying}.
In Scala, functions are allowed to have multiple parameter lists to make them curried, although uncurried functions are preferred for performance reasons.
Since these functions will be transformed from Scala code and back, it is desirable to maintain a high-level equivalence between these two representations.
Thus, \cref{fig:lambda-calculus} shows the $\lambda$-calculus extended to represent multi-argument functions using $n$-ary abstraction and application.

\begin{figure}
% annoying \enspace hack to get operators aligned along the centre
\begin{align*}
M, N &\mathrel{::=} x & \text{(variable)} \\
&\mathrel{\enspace\mid\enspace} \lambda(x_1, \ldots, x_n).M & \text{($n$-ary abstraction)} \\
&\mathrel{\enspace\mid\enspace} M(N_1, \ldots, N_n) & \text{($n$-ary application)}
\end{align*}
\caption{The untyped $\lambda$-calculus extended with $n$-ary abstraction and application.}
\label{fig:lambda-calculus}
\end{figure}

% TODO: maybe move this after normalisation
\subsubsection{The Function \textsc{adt}}
The syntactic representation of functions is thus represented as the uni-typed \scala{Function} \textsc{adt} in \cref{fig:function-adt}.
\scala{Defunc} is the higher-level representation of defunctionalised higher-order functions, which are desugared into \scala{Lambda} terms for normalisation.
\scala{Lambda} represents $n$-ary $\lambda$-terms, extended with the following:
\begin{itemize}
  \item Optional explicit type annotations for variables -- these are not used for type-checking, but are there to preserve Scala type annotations originally written by the user.
  \item \scala{Translucent} terms to encapsulate open terms holding a \scala{scala.meta.Term} which cannot be normalised further. These carry an environment of variable bindings to substitute back in during pretty-printing -- this is analogous to splicing into quoted expressions in the meta-programming world.
\end{itemize}

\begin{figure}
\begin{minted}{scala}
trait Function

// Core expression language
trait Lambda extends Function
case class Abs(xs: List[Var], f: Function) extends Lambda
case class App(f: Function, xs: List[Function]) extends Lambda
case class Var(name: VarName, displayType: Option[scala.meta.Type]) extends Lambda
case class Translucent(t: Term, env: Map[VarName, Function]) extends Lambda

// Defunctionalised higher-order functions
trait Defunc extends Function
case object Identity extends Defunc
case object Flip extends Defunc
case object Compose extends Defunc
...
\end{minted}
\caption{The \scala{Function} \textsc{adt} for representing functions.}
\label{fig:function-adt}
\end{figure}

The original intention was to represent \scala{Function} as a type-parameterised \textsc{adt} for improved type safety, where \scala{Lambda} would use a variant of the simply typed $\lambda$-calculus.
This would've also allowed \scala{Parser} to be parameterised by the result type of the parser.
However, attempting to implement this ran into two main hurdles:
\begin{itemize}
  \item \scala{Var} and \scala{Translucent} terms would need to be created with concrete type parameters of their inferred types. Scalafix's semantic \textsc{api} is not powerful enough to guarantee that all terms can be queried for their inferred types -- in fact, the built-in Scalafix rule \emph{Explicit Result Types} calls the Scala 2 presentation compiler to extract information like this\footnote{\url{https://github.com/scalacenter/scalafix/issues/1583}}. This solution is not ideal as it is brittle and breaks Scalafix's cross-compatibility promises.
  \item Scala 2's type inference for \textsc{gadt}s is less than ideal, requiring extra type annotations and unsafe casts which ultimately defeat the original purpose of type safety. This situation is improved, although not completely solved, in Dotty~\cite{parreaux_towards_2019} -- but Scalafix does not yet support Scala 3.
\end{itemize}

\subsection{Achieving Normalisation}\label{sec:normalisation-approach}
\TODO{
% TODO: this is based on NbE ideas!! move accordingly
Partial evaluation and normalisation are related concepts: it is useful to view normalisation as statically evaluating as many terms as possible, but since not all terms have known values, the expression cannot be fully evaluated to a result value.
Normalisation can thus be viewed simply as a process of evaluation, but in the presence of unknown terms.
This \namecref{sec:normalisation-approach} first explains the traditional notion of normalisation by rewriting, before introducing normalisation by evaluation as a more elegant and efficient alternative.
}

\subsubsection{Reduction-Based Normalisation}
In the $\lambda$-calculus, terms are reduced in size by applying $\beta$-reduction: \cref{fig:beta-reduction} shows how this can be defined for the $n$-ary $\lambda$-calculus.
Unlike the standard $\lambda$-calculus, reduction will only take place if the expected number of arguments in $\mathbf{x} = x_1, \ldots, x_n$ are equal to the number of arguments in $\mathbf{N} = N_1, \ldots, N_n$; otherwise, the term is left as is.
This is therefore a \emph{directed} notion of reduction, which can be implemented as a term-rewriting system, in a similar way to how \scala{Parser} terms are simplified.

The goal is to achieve beta normal form ($\beta$-\textsc{nf}) by allowing $\beta$-reduction to occur deep inside $\lambda$-terms, in all redexes of a term, until no more reductions can be made.
Normally in the untyped $\lambda$-calculus, $\beta$-reduction as a rewriting rule does not manage to be strongly normalising as desired, and may lead to non-terminating rewrites.
However, despite the \scala{Lambda} representation itself being untyped, all \scala{Lambda} terms are converted from valid typed Scala expressions.
Scala 3 is modelled on the theoretic foundations of the Dependent Object Types (\textsc{dot}) calculus, which has been shown to be strongly normalisable~\cite{wang_strong_2017}.
Although not proven for Scala 2, it is reasonable to assume that the same result holds, at least for the subset of expressions that are represented in the \scala{Lambda} representation.

\begin{figure}[htbp]
\begin{equation*}
(\lambda \mathbf{x} . M) \mathbf{N} \rightarrow_\beta M[\mathbf{N}/\mathbf{x}] \hspace{3em} (\text{if } | \mathbf{x} | = | \mathbf{N} | )
\end{equation*}
\caption{The $\beta$-reduction rule for the $n$-ary lambda calculus.}
\label{fig:beta-reduction}
\end{figure}

In \cref{fig:beta-reduction}, the syntax $M[N/x]$ denotes term substitution, where all free occurrences of $x$ in $M$ are replaced with $N$.
Substitution must avoid \emph{variable capture}, when $N$ contains free variables that are bound in the scope where $x$ is found~\cite{van-bakel_tsfpl_2022}.
Capture-avoiding substitution is a notoriously tricky and subtle problem.
There are a plethora of different approaches to solve this, mostly boiling down to how variable names get represented:

\paragraph{Naïve substitution}
Representing variable names as strings is the most straightforward approach in terms of understandability.
Naïvely substituting these terms, however, seems logically simple but can be very tricky to get right.
This approach requires calculating the free variables in a scope before performing substitution, renaming bound variables if it would lead to variable capture.
This is not used in any real implementation of the $\lambda$-calculus due to its inefficiency, as it requires traversing the term tree multiple times.

\paragraph{Barendregt's convention}
Renaming all bound variables to be unique satisfies \textbf{Barendregt's convention}~\cite{barendregt_lambda_1984}, which removes the need to check for variable capture during substitution.
However, variables still have to be renamed during substitution -- this administrative renaming has relatively high performance overhead and chews through a scarily large number of fresh variable names.
This approach can be massively optimised though: the Haskell \textsc{ghc} compiler uses a technique dubbed ``the Rapier''~\cite{peytonjones_secrets_2002}, maintaining invariants to avoid renaming on substitution when unnecessary.
Unfortunately, maintaining the invariants to keep this transformation correct becomes very difficult~\cite{maclaurin_thefoil_2023}.

\paragraph{Nameless and hybrid representations}
Nameless representations like \emph{De Bruijn indices}~\cite{debruijn_lambda_1972} eschew names entirely, instead representing variables as the number of binders between the variable and its binding site.
Although an elegant representation, De Bruijn terms are notoriously difficult to work with, as they are not easily human-readable.
% https://proofassistants.stackexchange.com/questions/900/when-should-i-use-de-bruijn-levels-instead-of-indices
Furthermore, performing substitutions with De Bruijn terms has an overhead as variable positions have to be shifted.
To avoid this, hybrid representations combining named and nameless representations exist~\cite{mcbride_imnotanumber_2004,chargueraud_locally_2012}, but they become rather complex solutions for what should be a relatively simple $\lambda$-calculus implementation for \texttt{parsley-garnish}'s needs.

\paragraph{Higher-order abstract syntax}
\emph{Higher-order abstract syntax} (\textsc{hoas})~\cite{pfenning_hoas_1988} sidesteps variable binders entirely by borrowing binding and substitution from the meta-language.
The previous techniques were examples of first-order abstract syntax, representing variables and unknowns with identifiers, whether with names or indices.
A simple \textsc{foas} approach would build an expression using identifiers for bound variables like so:
\begin{minted}{scala}
trait FOAS
case class Abs[A, B](x: Var[A], f: FOAS[B]) extends FOAS[A => B]
case class App[A, B](f: FOAS[A => B], x: FOAS[A]) extends FOAS[B]
case class Var[A](name: String) extends FOAS[A]

// λx. λy. add x y
val x = Var("x")
val y = Var("y")
val expr = Abs(x, Abs(y, App(App(Var("add"), x), y)))
\end{minted}
%
A \textsc{hoas} approach differs in the representation of lambda abstractions, so that bound variables do not get named but are instead represented using bindings in the meta-language (in this case, Scala):
\begin{minted}[texcomments]{scala}
trait HOAS
case class Abs[A, B](f: HOAS[A] => HOAS[B]) extends HOAS[A => B]
case class App[A, B](f: HOAS[A => B], x: HOAS[A]) extends HOAS[B]
case class Var[A](name: String) extends HOAS[A]

// λx. λy. add x y
val expr = Abs(x => Abs(y => App(App(Var("add"), x), y)))
\end{minted}
%
Therefore, \textsc{hoas} performs substitution through Scala's function application, which makes this extremely fast compared to the other approaches.
However, since lambda abstractions are represented as lambda expressions within Scala itself, the function body becomes wrapped under Scala's variable binding, making them difficult to work with.

% TODO we use this but with nbe, syntax - named, semantics - hoas, normalising by evaluation = automatically alpha converts terms by renaming
% \scala{Function} has one binding construct -- the n-ary abstraction. This introduces variables and the need for capture-avoiding substitution.

\subsubsection{Normalisation by Evaluation}
An interesting alternative approach stems from a notion of \emph{reduction-free} normalisation, based on an undirected notion of term equivalence, rather than directed reduction.
% TODO: this is good framing for using this to also see if parsers are equivalent
\emph{Normalisation by Evaluation} (\textsc{nbe}) evaluates syntactical terms into a semantic model, which is then \emph{reified} back into the syntactic model~\cite{filinski_nbe_2004}.
Normalisation is then just defined as the composition of these two operations, as illustrated in \cref{fig:nbe}.

\begin{figure}
\begin{equation*}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12cYAPHYAMoBPMDjo8AviAml0mXPkIoATOSq1GLNp259BMALZ1RWAMZwpE9TCgBzeEVAAzAE4QDSMiBwQkARmo4AAssJxwkAFpVDWZWRA4uXn4dJIYAIxc6UwBrKDSGCQACCMKUvTKcFwys3PzLWRBXd39qH09AkLDI6IY6NJgGAAV5PAI2Bhgu6npY7US+XGAXGFChSwoJIA
\begin{tikzcd}
  \text{Syntactic domain} \arrow[rr, "\text{\textlbrackdbl} - \text{\textrbrackdbl}", shift left=2] &  & \text{Semantic domain} \arrow[ll, "\textit{reify}", shift left=2]
\end{tikzcd}
\end{equation*}
\vspace{0.5ex}
\begin{equation*}
\textit{normalise} = \textit{reify} \circ \text{\textlbrackdbl} - \text{\textrbrackdbl}
\end{equation*}
\caption{Normalisation by evaluation in a semantic model.}
\label{fig:nbe}
\end{figure}

The denotational model (denoted by $\text{\textlbrackdbl} - \text{\textrbrackdbl}$) is specifically constructed to be \emph{residualising}, meaning that terms can be extracted out into the original syntactic representation.

\TODO{
  Represent semantic model with HOAS
}

\begin{minted}{scala}
trait HOAS
case class HAbs(n: Int, f: List[HOAS] => HOAS) extends HOAS
case class HApp(f: HOAS, xs: List[HOAS]) extends HOAS
case class HVar(name: VarName) extends HOAS
case class HTranslucent(t: Term, env: Map[VarName, HOAS]) extends HOAS
\end{minted}
%
Abs represents n-ary $\lambda$-abstraction; App is n-ary function application; Var represents opaque free variables.
Translucent is treated as opaque during normalisation, but contains normalisable terms in an environment, where they are substituted back in after normalisation.

% Use HOAS to leverage the meta-language (i.e. Scala) as our semantic model -- blazingly fast! (theoretically lol)
% No rewriting = no worrying about avoiding capture.
% Shift that burden onto the scala compiler.

\subsubsection{Evaluating Performance of Normalisation Strategies}

\subsection{Converting Scalameta Terms to the Function \textsc{adt}}
There are three cases to consider when converting \scala{scala.meta.Term} nodes into \scala{Function} terms.

\paragraph{Lambda Expressions}
Writing parsers often involves defining simple lambda expressions used to glue together parsers, or to transform the result of a parser:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(char => char.toInt)
\end{minted}

These lambda expressions are represented in the Scalameta \textsc{ast} as \scala{Term.Function} nodes, which are recursively traversed to collect all parameter lists.
This is folded into a chain of $n$-ary abstractions, with the final term being the body of the lambda, which is wrapped into a \scala{Translucent} term.
To ensure that the parameter names in the \scala{Translucent} body term are unique, the parameters are $\alpha$-converted to fresh names.
Take the following example to see why this is helpful:
% this is the "gensym" approach
\begin{minted}{scala}
  a => (a, b) => a + b
\end{minted}
Although any sane Scala programmer would not write this, this convoluted example shows how variable shadowing can occur -- the \scala{a} in the function body refers to the \scala{a} in the second parameter list, as it shadows the \scala{a} in the first parameter list.
The resulting \scala{Function} term would then resemble the following $\lambda$-calculus expression:
\begin{lstlisting}
  \(_l1). \(_l2, _l3). Translucent(%\textbf{\_l2 + \_l3}%, env = {%\textbf{\_l1}% -> _l1, %\textbf{\_l2}% -> _l2, %\textbf{\_l3}% -> _l3})
\end{lstlisting}
Values shown in bold are \scala{scala.meta.Term} nodes, so the lambda body's environment maps \scala{Term.Name} nodes to their corresponding variable terms.
When the term is pretty-printed, the \scala{Term.Name} nodes are replaced with the corresponding \scala{Function} terms -- this is similar to the splicing operation on quasiquotes.

\paragraph{Placeholder Syntax}
Scala supports a placeholder syntax using underscores to make lambda expressions more concise, so the earlier parser can be rewritten as:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(_.toInt)
\end{minted}

Scalameta differentiates between regular lambda expressions and those using placeholder syntax, representing the latter as \scala{Term.AnonymousFunction} nodes.
This makes it easy to identify which approach to be taken during conversion.
To convert this case, each successive underscore in the expression body is replaced with a fresh variable name.
Using placeholder syntax only results in a fully uncurried function with a single parameter list\footnote{\url{https://www.scala-lang.org/files/archive/spec/2.13/06-expressions.html#anonymous-functions}} -- this is filled with the freshly generated variable names in order of their occurrence in the expression body.

\paragraph{Eta-Expansion}
If the term is not a lambda expression, \texttt{parsley-garnish} attempts to $\eta$-expand the term if possible.
For example, an idiomatic parser written using the \emph{Parser Bridges} pattern~\cite{willis_design_2022} could resemble the following:
\begin{minted}{scala}
case class AsciiCode(code: Int)
object AsciiCode extends ParserBridge1[Char, AsciiCode] {
  def apply(char: Char): AsciiCode = AsciiCode(char.toInt)
}
val asciiCode = AsciiCode(item)
\end{minted}
%
When \texttt{parsley-garnish} converts \scala{asciiCode} to a \scala{Parser}, it desugars the bridge constructor into something resembling \scala{item.map(AsciiCode.apply)}.
The $\eta$-expanded form of \scala{AsciiCode.apply} would be as follows:
\begin{minted}{scala}
(char: Char) => AsciiCode.apply(char)
\end{minted}
%
To $\eta$-expand \scala{scala.meta.Term} nodes, \texttt{parsley-garnish} attempts to look up the method signature of its symbol using Scalafix's semantic \textsc{api}.
This is not always possible -- in that case, the term can't be statically inspected any further and is just wrapped in a \scala{Translucent} term.

\subsection{Normalising Function Terms}
\begin{figure}[htbp]
\begin{equation*}
% Created here (but modified slightly) https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDnGADx2AAJAPIBBAMoBfEJNLpMufIRQBmCtTpNW7Lj35CxUgHq6+AsADNps+djwEiZAEwaGLNok7czwAGLMwAGN7MGs5EAw7JSI1Fxo3bU9TfQAZegBbACMoejDbRQcUcnV4rQ8vPQFxGHT6MDxAuDyIhRCiYrjNdx1vfXEAT3r6XmsNGCgAc3giUAsAJwh0pGKQHAgkMi7EirMcARg3aRpGekyYRgAFVujPRhgLHBlw+cWNmjWkNS3y5L2BOZgWAs-SOIBOZ0u10KYPujxsIBeS0QTne60QKwSP16f2AAIsd2CoPB5yuUWhdweT1mCyRKNWaK+mJ6lT2wEgc1qjCwcBgkgABABePm-VkAoEg4UcQJYOaBSUs-aHSXS2Xy3b-e4EnD8kDHU4kqHKEBzLATAAWcOeNKQdI+iE2XLA5RycDN41133YyGQfIAtHzKJQZJRJEA
\begin{tikzcd}[sep=5em]
  \text{Semantics} & \text{HOAS} \arrow[rr, "\texttt{eval}"]                                                                                                                                         &  & \text{HOAS}^\text{nf} \arrow[dd, "\texttt{reify}"] \\
                   &                                                                                                                                                                                 &  &                                                    \\
  \text{Syntax}    & \text{Function} \arrow[uu, "\texttt{reflect}"] \arrow[rr, "\substack{\texttt{normalise} \\ =\ \texttt{reify}\ \circ\ \texttt{eval}\ \circ\ \texttt{reflect}}"'] \arrow[rruu, "{\text{\textlbrackdbl} - \text{\textrbrackdbl}}", dashed] &  & \text{Lambda}                                     
\end{tikzcd}
\end{equation*}
\caption{Normalisation by evaluation for the \scala{Function} datatype.}
\end{figure}

\TODO{the code below is outdated}
\subsubsection{Reflection}

\begin{minted}{scala}
trait Function {
  def reflect: HOAS = {
    def reflect0(func: Function, boundVars: Map[Var, HOAS]): HOAS = func match {
      case Abs(xs, f) =>
        HAbs(xs.size, vs => reflect0(f, boundVars ++ xs.zip(vs)))
      case App(f, xs) =>
        HApp(reflect0(f, boundVars), xs.map(reflect0(_, boundVars)))
      case v @ Var(name) =>
        boundVars.getOrElse(v, HVar(name))
      case Translucent(term, env) =>
        HTranslucent(term, env.mapValues(reflect0(_, boundVars)))

      case Id => HAbs(f => f)
      case Flip => HAbs(f => HAbs(x => HAbs(y => HApp(HApp(f, y), x))))
      case Compose => HAbs(f => HAbs(g => HAbs(x => HApp(f, HApp(g, x)))))
    }

    reflect0(this, Map.empty)
  }
}
\end{minted}

\subsubsection{Evaluation}
\begin{minted}{scala}
trait HOAS {
  def eval: HOAS = this match {
    case HAbs(n, f) => HAbs(n, x => f(x).eval)
    case HApp(f, x) => f.whnf match {
      case HAbs(_, g) => g(x).eval
      case g          => HApp(g.eval, x.map(_.eval))
    }
    case HTranslucent(t, env) => HTranslucent(t, env.map { case (k, v) => k -> v.eval })
    case HTranslucent(t, env) => HTranslucent(t, env.mapValues(_.eval))
    case _ => this
  }

  private def whnf: HOAS = this match {
    case HApp(f, x) => f.whnf match {
      case HAbs(_, g) => g(x).whnf
      case g          => HApp(g, x)
    }
    case HTranslucent(t, env) => HTranslucent(t, env.mapValues(_.whnf))
    case _ => this
  }
}
\end{minted}

\subsubsection{Reification}
\begin{minted}{scala}
trait HOAS {  
  def reify: Function = this match {
    case HAbs(n, f) =>
      val params = (1 to n).map(_ => Var.fresh()).toList
      Abs(params, f(params.map(x => HVar(x.name))).reify)
    case HApp(f, xs) => App(f.reify, xs.map(_.reify))
    case HVar(name) => Var(name)
    case HTranslucent(t, env) => Translucent(t, env.mapValues(_.reify))
  }
}
\end{minted}

\subsection{Converting Functions back to Scalameta Terms}
Surprise bitches same shit again, quasiquotes ftw

\subsection{Further Work?}
% TODO: probably move this to the end of the document
Eta reduction -- this is more complicated than in Haskell since Scala has special syntax % https://docs.scala-lang.org/scala3/book/fun-eta-expansion.html
Partial evaluation, not just normalisation (if we reduce to fully closed terms 1+1 can we get it to evaluate to 2? -- except currently this would be a Translucent term)

\TODO{
TODO:
Abstraction built over scalafix/meta ASTs to represent functions.
Allows us to statically evaluate function composition/flipping etc, so it doesn't turn into one big mess -- again, human readability of the transformed output is the goal.
% https://hugopeters.me/posts/16/ -- should I switch to de bruijn indices? Bound variables get instantiated only at the end, don't have to worry about alpha conversion (relation to Barendregt's convention if needed??)
Abstraction is again an ADT as a lambda calculus, but with parameter lists so not everything is curried.
\^ idk, this is still a work-in-progress. Seems that there might not be enough time to uncurry the leftrec analysis so this design decision might not be super important.
Representation as a lambda calc has allocation overhead, but greatly simplifies function evaluation via beta reduction, instead of having to deal with high-level representations of compose/id (not too bad tbh) and flip (annoying).
Also attempted to make it typed but that didn't go so well with Scala's limitations on type inference.

* Extracting method arguments (alongside their types) is very painful
* Need to unify information from signature (within symbolinformation) and synthetics
  * synthetics exist in certain cases: .apply methods, showing the concrete type of a generic argument, implicit conversions
    * from https://scalacenter.github.io/scalafix/docs/developers/semantic-tree.html: SemanticTree is a sealed data structure that encodes tree nodes that are generated by the compiler from inferred type parameters, implicit arguments, implicit conversions, inferred .apply and for-comprehensions.

* Don't have full access to type information - can do more work here theoretically, but its difficult and error-prone
* So we don't model a typed lambda calculus, just have it untyped

Approaches - AVOIDING capture via substitution
* Substitution approaches
  * De Bruijn indices - inefficient to open/close terms so much - De Bruijn levels as an alternative
  * HOAS
* Normalisation by evaluation
}

\end{document}
