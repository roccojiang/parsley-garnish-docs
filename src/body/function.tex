\documentclass[../../main.tex]{subfiles}

\begin{document}

\section{Function Representation}\label{sec:function-representation}
\TODO{
  This section is about simplifying in the general domain, so really Squid can do all of this? Still an interesting approach ig
  And shows a shortcoming of scalameta quasiquotes
}

\Cref{sec:parser-representation} showed that it is useful to lift \textsc{ast} nodes corresponding to parser terms into an intermediate \scala{Parser} representation.
Crucially, \cref{sec:simplify-parsers} demonstrates how this representation lends itself to reducing parsers to a simpler form.
This \namecref{sec:function-representation} extends this idea to representing functions in a similar, statically-inspectable manner, and explores why this machinery is not just desirable but necessary for \texttt{parsley-garnish}.

% TODO: there's some decent stuff in Ch8 of david davie's masters thesis briefly summing up how to articulate related things in a succinct manner
\subsection{Representing Functions}
Utilising parser laws as a basis for term simplification for the \scala{Parser} \textsc{adt} has managed to tame the jumble of parser combinators resulting from the left-recursing factoring transformation~\cref{sec:factor-leftrec}.
However, the functions passed to lifting combinators such as \scala{map} have so far been treated as opaque, and have not been subject to the same simplifications as parsers.

Continuing from the running example from the previous \namecref{sec:parser-representation}, the function passed to the \scala{map} combinator has ended up in an unnecessarily complex form:
\begin{minted}{scala}
val f: Function = flip(compose(a => b => a + b)(identity))
\end{minted}
%
This is because the transformations on parsers during left-recursion factoring involve mapping over parsers with higher-order functions such as \scala{flip} to reverse the order of arguments, and \scala{compose} to compose functions.
These are used to ensure that unfolded parsers are recombined in a lawful manner, but results in a lot of syntactic noise.
Yet again, leaving the transformed terms in this form would be unacceptable for the linter.
It is vital that these functions are partially evaluated and \emph{normalised} into a semantically equivalent but visually simpler form.
To achieve this, a representation of functions is needed that is amenable to the same kind of static inspection as the \scala{Parser} \textsc{adt}.

\subsubsection{Defunctionalisation}
In the same way that representing parsers as datatypes allows them to be statically inspectable, the aforementioned higher-order functions can also be rendered as datatypes to allow them to be analysed in a similar manner.
This is an example of \emph{defunctionalisation}, a technique to eliminate higher-order functions by transforming them into specialised first-order equivalents~\cite{reynolds_defunc_1972,danvy_defunctionalization_2001}.

Ideally for type safety, this could be represented as a generalised algebraic data type (\textsc{gadt})~\cite{cheney_gadt_2003} with a type parameter documenting the type that the function term represents:
\begin{minted}{scala}
trait Function[+T]
case class Identity[A]() extends Function[A => A]
case class Flip[A, B, C]() extends Function[(A => B => C) => B => A => C]
case class Compose[A, B, C]() extends Function[(B => C) => (A => B) => A => C]
\end{minted}

\subsubsection{Lambda Calculus}
The defunctionalised constructs alone are not enough to fully represent the functions passed to combinators.
There needs to be a way to apply them to arguments, and to represent the arguments themselves -- the core language to do so is the lambda calculus~\cite{church_lambda_1936}.
In the $\lambda$-calculus, each function only takes one argument, and multi-argument functions are represented as a chain of single-argument functions: this is known as \emph{currying}.
In Scala, functions are allowed to have multiple parameter lists to make them curried, although uncurried functions are preferred for performance reasons.
Since these functions will be transformed from Scala code and back, it is desirable to maintain a high-level equivalence between these two representations.
Thus, \cref{fig:lambda-calculus} shows the $\lambda$-calculus extended to represent multi-argument functions using $n$-ary abstraction and application.

\begin{figure}
% annoying \enspace hack to get operators aligned along the centre
\begin{align*}
M, N &\mathrel{::=} x & \text{(variable)} \\
&\mathrel{\enspace\mid\enspace} \lambda(x_1, \ldots, x_n).M & \text{($n$-ary abstraction)} \\
&\mathrel{\enspace\mid\enspace} M(N_1, \ldots, N_n) & \text{($n$-ary application)}
\end{align*}
\caption{The untyped $\lambda$-calculus extended with $n$-ary abstraction and application.}
\label{fig:lambda-calculus}
\end{figure}

\subsubsection{The Function \textsc{adt}}
The syntactic representation of functions is thus represented as the uni-typed \scala{Function} \textsc{adt} in \cref{fig:function-adt}.
The \scala{Lambda} trait is a subset of the entire \scala{Function} trait, representing $\lambda$-terms, with the addition of a \scala{Translucent} term to encapsulate open terms holding a \scala{scala.meta.Term} which cannot be evaluated further.

\begin{figure}
\begin{minted}{scala}
trait Function

// Core expression language
trait Lambda extends Function
case class Abs(xs: List[Var], f: Function) extends Lambda
case class App(f: Function, xs: List[Function]) extends Lambda
case class Var(name: VarName) extends Lambda
case class Translucent(t: Term, env: Map[VarName, Function]) extends Lambda

// Defunctionalised higher-order functions
case object Identity extends Function
case object Flip extends Function
case object Compose extends Function
...
\end{minted}
\caption{The \scala{Function} \textsc{adt} for representing functions.}
\label{fig:function-adt}
\end{figure}

The original intention was to represent \scala{Function} as a type-parameterised \textsc{adt} for improved type safety, where \scala{Lambda} would use a variant of the simply typed $\lambda$-calculus.
This would've also allowed \scala{Parser} to be parameterised by the result type of the parser.
However, attempting to implement this ran into two main hurdles:
\begin{itemize}
  \item \scala{Var} and \scala{Translucent} terms would need to be created with concrete type parameters of their inferred types. Scalafix's semantic \textsc{api} is not powerful enough to guarantee that all terms can be queried for their inferred types -- in fact, the built-in Scalafix rule \emph{Explicit Result Types} calls the Scala 2 presentation compiler to extract information like this\footnote{\url{https://github.com/scalacenter/scalafix/issues/1583}}. This solution is not ideal as it is brittle and breaks Scalafix's cross-compatibility promises.
  \item Scala 2's type inference for \textsc{gadt}s is less than ideal, requiring extra type annotations and unsafe casts which ultimately defeat the original purpose of type safety. This situation is improved, although not completely solved, in Dotty~\cite{parreaux_towards_2019} -- but Scalafix does not yet support Scala 3.
\end{itemize}

\subsection{Achieving Normalisation}\label{sec:normalisation-approach}
Partial evaluation and normalisation are related concepts: it is useful to view normalisation as statically evaluating as many terms as possible, but since not all terms have known values, the expression cannot be fully evaluated to a result value.
Normalisation can thus be viewed simply as a process of evaluation, but in the presence of unknown terms.
This \namecref{sec:normalisation-approach} first explains the traditional notion of normalisation by rewriting, before introducing normalisation by evaluation as a more elegant and efficient alternative.

\subsubsection{Normalisation by Rewriting}
Essentially term-rewriting, similar to how Parsers are simplified.
Strong reduction -- allow beta reduction to occur deep inside lambda terms, in all redexes of a term, until no more reductions can be made.
To beta normal form, not just beta-WHNF (weak head normal form) % TODO: does jamie's thesis have more elaboration on terminology
% TODO: cite https://inria.hal.science/inria-00434283/document ? for NbE but also some of the terminology used

Aside: Untyped lambda calculus is not strongly normalising, but these terms are converted from valid typed Scala programs.
Although not formally proven, it is reasonable to assume that these terms are a subset of the simple lambda calculus that are strongly normalising -- they won't be particularly complex.

\scala{Function} has one binding construct -- the n-ary abstraction. This introduces variables and the need for capture-avoiding substitution.
Capture-avoiding substitution is hard, many approaches e.g. de Bruijn indices (but inefficient to open/close terms so much, so the correct choice is levels).
* There is a dizzying array of existing literature and techniques on to represent binders without names
Attempted a barendregt's but the optimised form (not freshening on every substitution) might be subtly incorrect, not sure tbh.

\subsubsection{Normalisation by Evaluation}
Fortunately, there is a cheat to completely circumvent the nightmare of capture-avoiding substitution.
Unlike traditional normalisation techniques, NbE bypasses rewriting entirely, instead appealing to their denotational semantics. (fuck yeah bitches)
Interpret the term into a denotational model which can then be evaluated, and reify the result back into a term.
Use HOAS to leverage the meta-language (i.e. Scala) as our semantic model -- blazingly fast! (theoretically lol)
No rewriting = no worrying about avoiding capture.
Shift that burden onto the scala compiler.

\begin{figure}
\begin{equation*}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12cYAPHYAMoBPMDjo8AviAml0mXPkIoATOSq1GLNp259BMALZ1RWAMZwpE9TCgBzeEVAAzAE4QDSMiBwQkARmo4AAssJxwkAFpVDWZWRA4uXn4dJIYAIxc6UwBrKDSGCQACCMKUvTKcFwys3PzLWRBXd39qH09AkLDI6IY6NJgGAAV5PAI2Bhgu6npY7US+XGAXGFChSwoJIA
\begin{tikzcd}
  \text{Syntax} \arrow[rr, "\text{\textlbrackdbl} - \text{\textrbrackdbl}", shift left=2] &  & \text{Semantics} \arrow[ll, "\textit{reify}", shift left=2]
\end{tikzcd}
\end{equation*}
\vspace{1ex}
\begin{equation*}
\textit{normalise} = \textit{reify} \circ \text{\textlbrackdbl} - \text{\textrbrackdbl}
\end{equation*}
\caption{Normalisation by evaluation in a semantic model.}
\end{figure}

\subsection{Converting Scalameta Terms to the Function \textsc{adt}}
Three cases:
\begin{enumerate}
    \item \scala{Term.Function} nodes, representing lambda expressions
    \item \scala{Term.AnonymousFunction} nodes, representing lambda expressions using placeholder syntax
    \item Any other term, which will be $eta$-expanded if possible
\end{enumerate}

\subsubsection{Lambda Expressions}
Often found as relatively simple functions to glue together parsers, or transform the result of a parser:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(char => char.toInt)
\end{minted}

Choose a particularly convoluted example:
\begin{minted}{scala}
a => (a, b) => a + b
\end{minted}
In this example, the \scala{a} in the function body refers to the \scala{a} in the second parameter list, as it shadows the \scala{a} in the first parameter list.

Conversion ``flattens'' the lambda expression into a chain of n-ary abstractions, with the final term being the body of the lambda.
Because we lose scoping information this way, perform $\alpha$-conversion on variables.
% TODO: this is "gensym" approach
The body becomes a \scala{Translucent} term.

\begin{lstlisting}
\(_l1). \(_l2, _l3). Translucent(%\textbf{\_l2 + \_l3}%, env = {%\textbf{\_l1}% -> _l1, %\textbf{\_l2}% -> _l2, %\textbf{\_l3}% -> _l3})
\end{lstlisting}
Values in bold are Scalameta tree nodes, so the body term's environment maps \scala{Term.Name} nodes to their corresponding variable terms.

\subsubsection{Placeholder Syntax}
Scala supports placeholder syntax for terseness, so the earlier parser can be rewritten as:
\begin{minted}{scala}
val asciiCode: Parsley[Int] = item.map(_.toInt)
\end{minted}

\subsubsection{Eta-Expansion}
\begin{minted}{scala}
case class AsciiCode(code: Int)
object AsciiCode extends ParserBridge1[Char, AsciiCode] {
  def apply(char: Char): AsciiCode = AsciiCode(char.toInt)
}
val asciiCode = AsciiCode(item) // desugars to item.map(AsciiCode.apply)
\end{minted}

\subsection{Normalising Function Terms}
\begin{figure}[htbp]
\begin{equation*}
% Created here (but modified slightly) https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDnGADx2AAJAPIBBAMoBfEJNLpMufIRQBmCtTpNW7Lj35CxUgHq6+AsADNps+djwEiZAEwaGLNok7czwAGLMwAGN7MGs5EAw7JSI1Fxo3bU9TfQAZegBbACMoejDbRQcUcnV4rQ8vPQFxGHT6MDxAuDyIhRCiYrjNdx1vfXEAT3r6XmsNGCgAc3giUAsAJwh0pGKQHAgkMi7EirMcARg3aRpGekyYRgAFVujPRhgLHBlw+cWNmjWkNS3y5L2BOZgWAs-SOIBOZ0u10KYPujxsIBeS0QTne60QKwSP16f2AAIsd2CoPB5yuUWhdweT1mCyRKNWaK+mJ6lT2wEgc1qjCwcBgkgABABePm-VkAoEg4UcQJYOaBSUs-aHSXS2Xy3b-e4EnD8kDHU4kqHKEBzLATAAWcOeNKQdI+iE2XLA5RycDN41133YyGQfIAtHzKJQZJRJEA
\begin{tikzcd}[sep=5em]
  \text{Semantics} & \text{HOAS} \arrow[rr, "\texttt{eval}"]                                                                                                                                         &  & \text{HOAS}^\text{nf} \arrow[dd, "\texttt{reify}"] \\
                   &                                                                                                                                                                                 &  &                                                    \\
  \text{Syntax}    & \text{Function} \arrow[uu, "\texttt{reflect}"] \arrow[rr, "\substack{\texttt{normalise} \\ =\ \texttt{reify}\ \circ\ \texttt{eval}\ \circ\ \texttt{reflect}}"'] \arrow[rruu, "{\text{\textlbrackdbl} - \text{\textrbrackdbl}}", dashed] &  & \text{Lambda}                                     
\end{tikzcd}
\end{equation*}
\caption{Normalisation by evaluation for the \scala{Function} datatype.}
\end{figure}

\begin{minted}{scala}
trait HOAS
case class HAbs(n: Int, f: List[HOAS] => HOAS) extends HOAS
case class HApp(f: HOAS, xs: List[HOAS]) extends HOAS
case class HVar(name: VarName) extends HOAS
case class HTranslucent(t: Term, env: Map[VarName, HOAS]) extends HOAS
\end{minted}
%
Abs represents n-ary $\lambda$-abstraction; App is n-ary function application; Var represents opaque free variables.
Translucent is treated as opaque during normalisation, but contains normalisable terms in an environment, where they are substituted back in after normalisation.

\subsubsection{Reflection}

\begin{minted}{scala}
trait Function {
  def reflect: HOAS = {
    def reflect0(func: Function, boundVars: Map[Var, HOAS]): HOAS = func match {
      case Abs(xs, f) =>
        HAbs(xs.size, vs => reflect0(f, boundVars ++ xs.zip(vs)))
      case App(f, xs) =>
        HApp(reflect0(f, boundVars), xs.map(reflect0(_, boundVars)))
      case v @ Var(name) =>
        boundVars.getOrElse(v, HVar(name))
      case Translucent(term, env) =>
        HTranslucent(term, env.mapValues(reflect0(_, boundVars)))

      case Id => HAbs(f => f)
      case Flip => HAbs(f => HAbs(x => HAbs(y => HApp(HApp(f, y), x))))
      case Compose => HAbs(f => HAbs(g => HAbs(x => HApp(f, HApp(g, x)))))
    }

    reflect0(this, Map.empty)
  }
}
\end{minted}

\subsubsection{Evaluation}
\begin{minted}{scala}
trait HOAS {
  def eval: HOAS = this match {
    case HAbs(n, f) => HAbs(n, x => f(x).eval)
    case HApp(f, x) => f.whnf match {
      case HAbs(_, g) => g(x).eval
      case g          => HApp(g.eval, x.map(_.eval))
    }
    case HTranslucent(t, env) => HTranslucent(t, env.map { case (k, v) => k -> v.eval })
    case HTranslucent(t, env) => HTranslucent(t, env.mapValues(_.eval))
    case _ => this
  }

  private def whnf: HOAS = this match {
    case HApp(f, x) => f.whnf match {
      case HAbs(_, g) => g(x).whnf
      case g          => HApp(g, x)
    }
    case HTranslucent(t, env) => HTranslucent(t, env.mapValues(_.whnf))
    case _ => this
  }
}
\end{minted}

\subsubsection{Reification}
\begin{minted}{scala}
trait HOAS {  
  def reify: Function = this match {
    case HAbs(n, f) =>
      val params = (1 to n).map(_ => Var.fresh()).toList
      Abs(params, f(params.map(x => HVar(x.name))).reify)
    case HApp(f, xs) => App(f.reify, xs.map(_.reify))
    case HVar(name) => Var(name)
    case HTranslucent(t, env) => Translucent(t, env.mapValues(_.reify))
  }
}
\end{minted}

\subsection{Converting Functions back to Scalameta Terms}
Surprise bitches same shit again, quasiquotes ftw

\subsection{Further Work?}
% TODO: probably move this to the end of the document
Eta reduction -- this is more complicated than in Haskell since Scala has special syntax % https://docs.scala-lang.org/scala3/book/fun-eta-expansion.html
Partial evaluation, not just normalisation (if we reduce to fully closed terms 1+1 can we get it to evaluate to 2? -- except currently this would be a Translucent term)

\TODO{
TODO:
Abstraction built over scalafix/meta ASTs to represent functions.
Allows us to statically evaluate function composition/flipping etc, so it doesn't turn into one big mess -- again, human readability of the transformed output is the goal.
% https://hugopeters.me/posts/16/ -- should I switch to de bruijn indices? Bound variables get instantiated only at the end, don't have to worry about alpha conversion (relation to Barendregt's convention if needed??)
Abstraction is again an ADT as a lambda calculus, but with parameter lists so not everything is curried.
\^ idk, this is still a work-in-progress. Seems that there might not be enough time to uncurry the leftrec analysis so this design decision might not be super important.
Representation as a lambda calc has allocation overhead, but greatly simplifies function evaluation via beta reduction, instead of having to deal with high-level representations of compose/id (not too bad tbh) and flip (annoying).
Also attempted to make it typed but that didn't go so well with Scala's limitations on type inference.

* Extracting method arguments (alongside their types) is very painful
* Need to unify information from signature (within symbolinformation) and synthetics
  * synthetics exist in certain cases: .apply methods, showing the concrete type of a generic argument, implicit conversions
    * from https://scalacenter.github.io/scalafix/docs/developers/semantic-tree.html: SemanticTree is a sealed data structure that encodes tree nodes that are generated by the compiler from inferred type parameters, implicit arguments, implicit conversions, inferred .apply and for-comprehensions.

* Don't have full access to type information - can do more work here theoretically, but its difficult and error-prone
* So we don't model a typed lambda calculus, just have it untyped

Approaches - AVOIDING capture via substitution
* Substitution approaches
  * De Bruijn indices - inefficient to open/close terms so much - De Bruijn levels as an alternative
  * HOAS
* Normalisation by evaluation
}

\end{document}
